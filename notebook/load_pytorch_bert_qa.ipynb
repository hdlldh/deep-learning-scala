{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load your own PyTorch BERT model\n",
    "\n",
    "In the previous [example](https://github.com/deepjavalibrary/djl/blob/master/jupyter/BERTQA.ipynb), you run BERT inference with the model from Model Zoo. You can also load the model on your own pre-trained BERT and use custom classes as the input and output.\n",
    "\n",
    "In general, the PyTorch BERT model from [HuggingFace](https://github.com/huggingface/transformers) requires these three inputs:\n",
    "\n",
    "- word indices: The index of each word in a sentence\n",
    "- word types: The type index of the word.\n",
    "- attention mask: The mask indicates to the model which tokens should be attended to, and which should not after batching sequence together.\n",
    "\n",
    "We will dive deep into these details later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                        \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`ai.djl:api:0.17.0`\n",
    "import $ivy.`ai.djl.pytorch:pytorch-model-zoo:0.17.0`\n",
    "import $ivy.`ai.djl.pytorch:pytorch-engine:0.17.0`\n",
    "import $ivy.`org.slf4j:slf4j-api:1.7.36`\n",
    "import $ivy.`org.slf4j:slf4j-simple:1.7.36`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import java packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.IOException\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.nio.file.Paths\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mai.djl.modality.nlp.DefaultVocabulary\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mai.djl.modality.nlp.bert.BertTokenizer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mai.djl.modality.nlp.qa.QAInput\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mai.djl.ndarray.NDList\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mai.djl.repository.zoo.Criteria\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mai.djl.training.util.{DownloadUtils, ProgressBar}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mai.djl.translate.{Batchifier, Translator, TranslatorContext}\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.IOException\n",
    "import java.nio.file.Paths\n",
    "import java.util\n",
    "\n",
    "import ai.djl.modality.nlp.DefaultVocabulary\n",
    "import ai.djl.modality.nlp.bert.BertTokenizer\n",
    "import ai.djl.modality.nlp.qa.QAInput\n",
    "import ai.djl.ndarray.NDList\n",
    "import ai.djl.repository.zoo.Criteria\n",
    "import ai.djl.training.util.{DownloadUtils, ProgressBar}\n",
    "import ai.djl.translate.{Batchifier, Translator, TranslatorContext}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model and vocab files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DownloadUtils.download(\n",
    "  \"https://djl-ai.s3.amazonaws.com/mlrepo/model/nlp/question_answer/ai/djl/pytorch/bertqa/0.0.1/bert-base-uncased-vocab.txt.gz\",\n",
    "  \"build/pytorch/bert-qa/vocab.txt\",\n",
    "  new ProgressBar\n",
    ")\n",
    "\n",
    "DownloadUtils.download(\n",
    "  \"https://djl-ai.s3.amazonaws.com/mlrepo/model/nlp/question_answer/ai/djl/pytorch/bertqa/0.0.1/trace_bertqa.pt.gz\",\n",
    "  \"build/pytorch/bert-qa/bert-qa.pt\",\n",
    "  new ProgressBar\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mBertQuAnTranslator\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BertQuAnTranslator extends Translator[QAInput, String] {\n",
    "\n",
    "  private var vocabulary: DefaultVocabulary = _\n",
    "  private var tokenizer: BertTokenizer = _\n",
    "  private var tokenList: util.List[String] = _\n",
    "\n",
    "  override def prepare(ctx: TranslatorContext): Unit = {\n",
    "    val path = Paths.get(\"build/pytorch/bert-qa/vocab.txt\")\n",
    "    vocabulary = DefaultVocabulary.builder\n",
    "      .optMinFrequency(1)\n",
    "      .addFromTextFile(path)\n",
    "      .optUnknownToken(\"[UNK]\")\n",
    "      .build\n",
    "    tokenizer = new BertTokenizer()\n",
    "  }\n",
    "\n",
    "  override def processInput(ctx: TranslatorContext, input: QAInput): NDList = {\n",
    "    val token = tokenizer.encode(input.getQuestion.toLowerCase, input.getParagraph.toLowerCase, 384)\n",
    "    // get the encoded tokens that would be used in precessOutput\n",
    "    tokenList = token.getTokens\n",
    "    // map the tokens(String) to indices(long)\n",
    "\n",
    "    val manager = ctx.getNDManager\n",
    "    val indices = tokenList.stream().mapToLong(vocabulary.getIndex).toArray\n",
    "    val attentionMask = token.getAttentionMask.stream().mapToLong(i => i).toArray\n",
    "    val tokenType = token.getTokenTypes.stream().mapToLong(i => i).toArray\n",
    "    val indicesArray = manager.create(indices)\n",
    "    val attentionMaskArray = manager.create(attentionMask)\n",
    "    val tokenTypeArray = manager.create(tokenType)\n",
    "\n",
    "    new NDList(indicesArray, attentionMaskArray, tokenTypeArray)\n",
    "  }\n",
    "\n",
    "  override def processOutput(ctx: TranslatorContext, list: NDList): String = {\n",
    "    val startLogits = list.get(0)\n",
    "    val endLogits = list.get(1)\n",
    "    val startIdx = startLogits.argMax().getLong().asInstanceOf[Int]\n",
    "    val endIdx = endLogits.argMax().getLong().asInstanceOf[Int]\n",
    "    tokenList.subList(startIdx, endIdx + 1).toString\n",
    "  }\n",
    "\n",
    "  override def getBatchifier: Batchifier = Batchifier.STACK\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:     100% |████████████████████████████████████████|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[scala-interpreter-1] INFO ai.djl.pytorch.engine.PtEngine - Number of inter-op threads is 4\n",
      "[scala-interpreter-1] INFO ai.djl.pytorch.engine.PtEngine - Number of intra-op threads is 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtranslator\u001b[39m: \u001b[32mBertQuAnTranslator\u001b[39m = ammonite.$sess.cmd3$Helper$BertQuAnTranslator@221769bc\n",
       "\u001b[36mcriteria\u001b[39m: \u001b[32mCriteria\u001b[39m[\u001b[32mQAInput\u001b[39m, \u001b[32mString\u001b[39m] = Criteria:\n",
       "\tApplication: UNDEFINED\n",
       "\tInput: class ai.djl.modality.nlp.qa.QAInput\n",
       "\tOutput: class java.lang.String\n",
       "\tModelZoo: ai.djl.localmodelzoo\n",
       "\n",
       "\u001b[36mmodel\u001b[39m: \u001b[32mai\u001b[39m.\u001b[32mdjl\u001b[39m.\u001b[32mrepository\u001b[39m.\u001b[32mzoo\u001b[39m.\u001b[32mZooModel\u001b[39m[\u001b[32mQAInput\u001b[39m, \u001b[32mString\u001b[39m] = ai.djl.repository.zoo.ZooModel@7a1ac919\n",
       "\u001b[36mpredictor\u001b[39m: \u001b[32mai\u001b[39m.\u001b[32mdjl\u001b[39m.\u001b[32minference\u001b[39m.\u001b[32mPredictor\u001b[39m[\u001b[32mQAInput\u001b[39m, \u001b[32mString\u001b[39m] = ai.djl.inference.Predictor@547dbfd"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val translator = new BertQuAnTranslator()\n",
    "val criteria = Criteria.builder\n",
    "  .setTypes(classOf[QAInput], classOf[String])\n",
    "  .optModelPath(Paths.get(\"build/pytorch/bert-qa/\"))\n",
    "  .optTranslator(translator)\n",
    "  .optProgress(new ProgressBar)\n",
    "  .build\n",
    "\n",
    "val model = criteria.loadModel()\n",
    "val predictor = model.newPredictor(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mquestion\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"When did BBC Japan start broadcasting?\"\u001b[39m\n",
       "\u001b[36mcontext\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"BBC Japan was a general entertainment Channel.\n",
       "Which operated between December 2004 and April 2006.\n",
       "It ceased operations after its Japanese distributor folded.\"\"\"\u001b[39m\n",
       "\u001b[36minput\u001b[39m: \u001b[32mQAInput\u001b[39m = ai.djl.modality.nlp.qa.QAInput@2cb677a9\n",
       "\u001b[36mpredictResult\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"[december, 2004]\"\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val question = \"When did BBC Japan start broadcasting?\";\n",
    "val context = \"BBC Japan was a general entertainment Channel.\\n\" +\n",
    "    \"Which operated between December 2004 and April 2006.\\n\" +\n",
    "    \"It ceased operations after its Japanese distributor folded.\";\n",
    "val input = new QAInput(question, context);\n",
    "val predictResult = predictor.predict(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A little more complicated example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcontext\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\\\"Norman\\\" comes from \\\"Norseman\\\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\"\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val context = \"\"\"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mq1\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"When were the Normans in Normandy?\"\u001b[39m\n",
       "\u001b[36mq2\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"In what country is Normandy located?\"\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val q1 = \"When were the Normans in Normandy?\"\n",
    "val q2 = \"In what country is Normandy located?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres8\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"[10th, and, 11th, centuries]\"\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(new QAInput(q1, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres9\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"[france]\"\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(new QAInput(q2, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
